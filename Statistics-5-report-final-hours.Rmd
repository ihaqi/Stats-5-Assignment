---
title             : "Assignment Statistics 5"
shorttitle        : "Bayesian hierarchical modeling "

author: 
  - name          : "Gustavo Villca Ponce"
    affiliation   : "1"
    corresponding : no
    email         : "gustavo.villcaponce@student.kuleuven.be"
  - name          : "MohammadHossein Haqiqatkhah"
    affiliation   : "2"
    corresponding : no
    email         : "mh.haqiqatkhah@student.kuleuven.be"
  - name          : "Sigert Ariens"
    affiliation   : "3"
    corresponding : no
    email         : "sigert.ariens@student.kuleuven.be"
  - name          : "Bavo Kempen"
    affiliation   : "4"
    corresponding : no
    email         : "bavo.kempen@student.kuleuven.be"


affiliation:
  - id            : "1"
    institution   : "r0292033"
  - id            : "2"
    institution   : "r0607671"
  - id            : "3"
    institution   : "r0446864"
  - id            : "4"
    institution   : "r0585283"
  - id            : ""
    institution   : "Faculty of Psychology and Educational Sciences, KU Leuven."
  
wordcount         : "1613"

bibliography      : ["r-references.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---

```{r setup, include=FALSE}
#rm(list=ls())
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
list.of.packages <- c("knitr","rjags","runjags", "haven","psych","car","magicfor","lattice", "papaja")
new.packages <- list.of.packages[!(list.of.packages %in%installed.packages()[,"Package"])]
if(length(new.packages)){install.packages(new.packages,repos = "http://cran.us.r-project.org")}
lapply(list.of.packages, require, character.only = TRUE)

set.seed(777)
Mywd<- getwd()
setwd(Mywd)

load.rather.than.run <- TRUE

```
```{r writing bib file, echo=FALSE,message=FALSE,warning=FALSE}
 references = "
@article{lucas2003reexamining,
  title={Reexamining adaptation and the set point model of happiness: reactions to changes in marital status.},
  author={Lucas, Richard E and Clark, Andrew E and Georgellis, Yannis and Diener, Ed},
  journal={Journal of personality and social psychology},
  volume={84},
  number={3},
  pages={527},
  year={2003},
  publisher={US: American Psychological Association}
}

  " 
  # Write out modelString to a text file
  writeLines( references , con="r-references.bib" )
```
```{r datapreparation, include=FALSE, cache=TRUE}
## Unique ID's
my.normalize <- function(x) {
  return ((x - mean(x, na.rm = TRUE)) / (sd(x, na.rm = TRUE)))
  }

set.seed(777)
#rm(list=ls())

data.raw <- read.csv("soep_statV.csv", header = TRUE, sep = ",", dec = ".")
data.raw.tmp <- transform(data.raw, pp=match(id,unique(id)))
data.recoded <- data.raw.tmp
data.recoded$emplStat55[data.raw.tmp$emplStat55!=1] <- 0

pp <- data.recoded$pp
norm.ls <- my.normalize(data.recoded$lifeSat) # ls: life satisfaction
norm.a <- my.normalize(data.recoded$age-55) #  age
norm.e <- data.recoded$emplStat55 # e: is he full-time employed?
norm.h <- my.normalize(data.recoded$healthSat) #  h: health
norm.p <- data.recoded$livtog # p: partneered?

data.norm <- as.data.frame(cbind(pp,norm.ls,norm.a,norm.h,norm.p,norm.e))
hist(data.raw$healthSat)
mean(data.raw$healthSat)
save(data.norm, file = "data.norm.Rdata")


Nsubjects <- length(unique(data.norm[,1]))
Ntotal <- length(data.norm[,1])

yName = "norm.LS" 
xName = c("norm.Age", "norm.Health", "norm.Partner","norm.Empl")
y1 <- data.norm[2]
y <- as.matrix(y1)
x = as.matrix(data.norm[3:6],ncol=length(xName))

Empl = x[,4]
Age = x[,1] 
Health = x[,2]
Partner = x[,3]
pp = unique(Age)

AgeXHealth = data.norm[, "norm.a"]*data.norm[, "norm.h"]
AgeXPartner = data.norm[, "norm.a"]*data.norm[, "norm.p"]
AgeXEmploy = data.norm[, "norm.a"]*data.norm[, "norm.e"]
HealthXEmploy = data.norm[, "norm.h"]*data.norm[, "norm.e"]
AgeXHealthXEmploy = data.norm[,"norm.a"]*data.norm[,"norm.h"]*data.norm[,"norm.h"]

```


```{r exploratory plots}
clean.data <- transform(data.raw, pp=match(id,unique(id)))

##centering
clean.data$age <-data.raw$age-55

##recoding some variables
clean.data$emplStat55[clean.data$emplStat55 == 2] <- 0
clean.data$emplStat55[clean.data$emplStat55 == 3] <- 0

variable.names <- list("id","ls", "age", "employ","health","partner","pp")
colnames(clean.data) <- variable.names


mean.table1=tapply(clean.data$ls,list(clean.data$age, clean.data$partner),mean)
mean.table3=tapply(clean.data$ls,list(clean.data$age, clean.data$employ),mean)
mean.table4=tapply(clean.data$ls,list(clean.data$age, clean.data$health),mean)

# life satisfaction as a function of Living together and age
colors=c("black","red")
plot(unique(clean.data$age),mean.table1[,1],type="b",xlim=c(0,5),ylim=c(0,10),xlab="Age (in years)",ylab="Life Satisfaction",axes=F,main="Life satisfaction as a function of Living together and age")
axis(side=1,at=c(0,1,2,3,4,5),labels=c(55,56,57,58,59,60))
axis(side=2,at=seq(0,10,1))
box()
points(unique(clean.data$age),mean.table1[,2],type="b",col="red")
legend(-0.180, 2, x.intersp = 0.12, legend = colnames(mean.table1), fill = colors, horiz = TRUE)
# life satisfaction as a function of employement and age

colors=c("black","red")
plot(unique(clean.data$age),mean.table3[,1],type="b",xlim=c(0,5),ylim=c(0,10),xlab="Age (in years)",ylab="Life Satisfaction",axes=F,main="Life satisfaction as a function of employement and age")
axis(side=1,at=c(0,1,2,3,4,5),labels=c(55,56,57,58,59,60))
axis(side=2,at=seq(0,10,1))
box()
points(unique(clean.data$age),mean.table3[,2],type="b",col="red")
legend(-0.180, 2, x.intersp = 0.12, legend = colnames(mean.table3), fill = colors, horiz = TRUE)
# life satisfaction as a function of health status and age

par(xpd=TRUE)
colors=rainbow(10)
plot(unique(clean.data$age),mean.table4[,1],type="n",xlim=c(0,5),ylim=c(0,10),xlab="Age (in years)",ylab="Life satisfaction",axes=F,main="life satisfaction as a function of health status and age")
axis(side=1,at=c(0,1,2,3,4,5),labels=c(55,56,57,58,59,60))
axis(side=2,at=seq(0,10,1))
box()
for(i in 1:length(colors)){
  points(unique(clean.data$age),mean.table4[,1+i],type="b",col=colors[i])
  legend(-0.180, 2, x.intersp = 0.12, legend = colnames(mean.table4), fill = colors, horiz=TRUE)
}
```


```{r JAGS specifics}
nchains = 2
nadapt = 1000 #many uninformative dispersion parameters
nburn = 1000
niter = 2000
myinits <- NULL
```


```{r main model 1 Model with correlations and relevant}
#Preperation

x = as.matrix(data.norm[3:6],ncol=length(xName))
x = cbind(x, AgeXHealth, AgeXPartner, AgeXEmploy, HealthXEmploy)


Nx = dim(x)[2]
Nx0 = dim(x)[2]-3 #Change for between subject madness

dataList.I.COR = list(
    x = x ,
    y = y ,
    pp = pp ,
    Ntotal = Ntotal ,
    Nsubjects = Nsubjects,
    Nx0 = Nx0,
    Nx= Nx,
    omega = diag(c(1,1,1)),
    mean = c(0,0,0),
    df = 3+1,
    prec = diag(c(1.0E-6,1.0E-6,1.0E-6))
  )
```


```{r main model 1 Models With all Correlation and relevant interactions}
modelString = "
model {
for ( i in 1:Ntotal ) {
	y[i,1] ~ dnorm( beta[pp[i],1] + beta[pp[i],2]*x[i, 2]+beta[pp[i],3]*x[i,3]+ sum( betax[1:Nx0] * x[i,(Nx-Nx0+1):Nx]), 1/sigma^2 )
} 
 # Don't forget to set your between subjects to the end of X! Dont forget that the first beta is beta0

#Betas
for(j in 1:Nsubjects){
beta[j, 1:3] ~ dmnorm( betamu , M ) 
}
 #Priors vague:
for (q in 1:Nx0){
  betax[q] ~ dnorm( 0 , 1/(10)^5)
}

  betamu[1:3] ~ dmnorm(mean, prec)

  M[1 : 3, 1 : 3] ~ dwish(omega, df)
  MA <- inverse(M)
  
  for (z in 1:3){ #variance to sd
  ss[z] <- sqrt(MA[z,z])
  }
  cor12 <- MA[1,2]/ss[1]*ss[2]
  cor13 <- MA[1,3]/ss[1]*ss[3]
  cor23 <- MA[2,3]/ss[2]*ss[3]

  sigma ~ dunif( 1.0E-3 , 1.0E+3 )  

}
"

writeLines(modelString , con="model_full_correlation" )
```


```{r main model 1 Running Jags correlation}

if(!load.rather.than.run){
  
  parameters.C.I <- c("ss", "cor12","cor13","cor23","betamu", "betax")

  out.model.COR <- run.jags(model = "model_full_correlation", monitor = parameters.C.I, data = dataList.I.COR, inits = myinits, n.chains = nchains, adapt = nadapt, burnin = nburn, sample = niter, thin = 180)

  model.1.main <- out.model.COR
}

if(load.rather.than.run){load("model.1.main.rdata")}
```

```{r main model 2 Model with refined correlations and interactions}
#Preperation

x = as.matrix(data.norm[3:6],ncol=length(xName))
x = cbind(x, AgeXHealthXEmploy)


Nx = dim(x)[2]
Nx0 = dim(x)[2]-3 #Change for between subject madness
meana <- runif(1, min=-10, max=10)
meanb <- runif(1, min=-10, max=10)

dataList.I.COR2 = list(
    x = x ,
    y = y ,
    pp = pp ,
    Ntotal = Ntotal ,
    Nsubjects = Nsubjects,
    Nx0 = Nx0,
    Nx= Nx,
    omega = diag(c(1,1)),
    mean = c(0,0),
    df = 2+1,
    prec = diag(c(1.0E-6,1.0E-6))
  )
```

```{r main model 2 Models With refined Correlation and refined interactions}
modelString = "
model {
for ( i in 1:Ntotal ) {
	y[i,1] ~ dnorm( beta[pp[i],1] + beta[pp[i],2]*x[i, 3]+beta2[pp[i]]*x[i,2]+ sum( betax[1:Nx0] * x[i,(Nx-Nx0+1):Nx]), 1/sigma^2 )
} 
 # Don't forget to set your between subjects to the end of X! Dont forget that the first beta is beta0

#Betas
for(j in 1:Nsubjects){
beta[j, 1:2] ~ dmnorm( betamu , M )
beta2[j] ~ dnorm( betamu2 , 1/(10)^5)
}
 #Priors vague:
for (q in 1:Nx0){
  betax[q] ~ dnorm( 0 , 1/(10)^5)
}

  betamu[1:2] ~ dmnorm(mean, prec)
  betamu2 ~ dnorm(0, 1/(10)^5)

  M[1 : 2, 1 : 2] ~ dwish(omega, df)
  MA <- inverse(M)
  
  for (z in 1:2){ #variance to sd
  ss[z] <- sqrt(MA[z,z])
  }
  cor12 <- MA[1,2]/ss[1]*ss[2]

  sigma ~ dunif( 1.0E-3 , 1.0E+3 )  
Empl = x[,4]
Age = x[,1] 
Health = x[,2]
Partner = x[,3]
}
"

writeLines(modelString , con="model_full_correlation_2" )
```



```{r main model 2 Running Jags correlation final}
if(!load.rather.than.run){
  parameters.C.I.2 <- c("ss", "cor12","betamu", "betax")

  out.model.COR2 <- run.jags(model = "model_full_correlation_2", monitor = parameters.C.I.2, data = dataList.I.COR2, inits = myinits, n.chains = nchains, adapt = nadapt, burnin = nburn, sample = niter, thin = 1)
  
  summary(out.model.COR2)

  model.1.main <- out.model.COR2
}

if(load.rather.than.run){load("model.2.main.rdata")}

```




```{r sens model 1  Models With refined Correlation and refined interactions}
modelString = "
model {
for ( i in 1:Ntotal ) {
	y[i,1] ~ dnorm( beta[pp[i],1] + beta[pp[i],2]*x[i, 2]+beta[pp[i],3]*x[i,3]+ sum( betax[1:Nx0] * x[i,(Nx-Nx0+1):Nx]), 1/sigma^2 )
} 
 # Don't forget to set your between subjects to the end of X! Dont forget that the first beta is beta0

#Betas
for(j in 1:Nsubjects){
beta[j, 1:3] ~ dmnorm( betamu , M ) 
}
 #Priors vague:
for (q in 1:Nx0){
  betax[q] ~ dnorm( rand[q] , 1/(10)^2)
}


  betamu[1:3] ~ dmnorm(mean, prec)

  M[1 : 3, 1 : 3] ~ dwish(omega, df)
  MA <- inverse(M)
  
  for (z in 1:3){ #variance to sd
  ss[z] <- sqrt(MA[z,z])
  }
  cor12 <- MA[1,2]/ss[1]*ss[2]
  cor13 <- MA[1,3]/ss[1]*ss[3]
  cor23 <- MA[2,3]/ss[2]*ss[3]

  sigma ~ dunif( 1.0E-1 , 1.0E+1 )  

}
"

writeLines(modelString , con="model_full_correlation" )
```


```{r sens model 2  Models With refined Correlation and refined interactions}
modelString.sens = "
model {
for ( i in 1:Ntotal ) {
	y[i,1] ~ dnorm( beta[pp[i],1] + beta[pp[i],2]*x[i, 3]+beta2[pp[i]]*x[i,2]+ sum( betax[1:Nx0] * x[i,(Nx-Nx0+1):Nx]), 1/sigma^2 )
} 
 # Don't forget to set your between subjects to the end of X! Dont forget that the first beta is beta0

#Betas
for(j in 1:Nsubjects){
beta[j, 1:2] ~ dmnorm( betamu , M )
beta2[j] ~ dnorm( betamu2 , 1/(10)^2)
}
 #Priors vague:
for (q in 1:Nx0){
  betax[q] ~ dnorm( rand , 1/(10)^2)
}

rand ~ dunif(-10, 10)
rand2 ~ dunif(-10, 10) 
  betamu[1:2] ~ dmnorm(mean, prec)
  betamu2 ~ dnorm(rand2, 1/(10)^2)

  M[1 : 2, 1 : 2] ~ dwish(omega, df)
  MA <- inverse(M)
  
  for (z in 1:2){ #variance to sd
  ss[z] <- sqrt(MA[z,z])
  }
  cor12 <- MA[1,2]/ss[1]*ss[2]

  sigma ~ dunif( 1.0E-1 , 1.0E+1 )  

}
"

writeLines(modelString.sens , con="model_full_correlation_2.sens" )
```

```{r sens model 1 Model with correlations and refined interactions2}
#Preperation
AgeXHealth = data.norm[, "norm.a"]*data.norm[, "norm.h"]
AgeXPartner = data.norm[, "norm.a"]*data.norm[, "norm.p"]
AgeXEmploy = data.norm[, "norm.a"]*data.norm[, "norm.e"]
HealthXEmploy = data.norm[, "norm.h"]*data.norm[, "norm.e"]
AgeXHealthXEmploy = data.norm[,"norm.a"]*data.norm[,"norm.h"]*data.norm[,"norm.h"]
x = as.matrix(data.norm[3:6],ncol=length(xName))
x = cbind(x, HealthXEmploy,AgeXPartner,AgeXHealth, AgeXEmploy,AgeXHealthXEmploy)

Nx = dim(x)[2]
Nx0 = dim(x)[2]-3 #Change for between subject madness


rand <- c()

mean1 <- runif(1,-5,5)
mean2 <- runif(1,-5,5)
mean3 <- runif(1,-5,5)
for (b in 1:Nx0){
  rand[b] <- runif(1,-5,5)
}

dataList.I.COR = list(
    x = x ,
    y = y ,
    pp = pp ,
    Ntotal = Ntotal ,
    Nsubjects = Nsubjects,
    Nx0 = Nx0,
    Nx= Nx,
    omega = diag(c(1,1,1)),
    mean = c(mean1,mean2,mean3),
    df = 2+1,
    prec = diag(c(1.0E-6,1.0E-6,1.0E-6)),
    rand = rand
  )
```


```{r sens model 2 Model with correlations and refined interactions2}
#Preperation
AgeXHealth = data.norm[, "norm.a"]*data.norm[, "norm.h"]
AgeXPartner = data.norm[, "norm.a"]*data.norm[, "norm.p"]
AgeXEmploy = data.norm[, "norm.a"]*data.norm[, "norm.e"]
HealthXEmploy = data.norm[, "norm.h"]*data.norm[, "norm.e"]
AgeXHealthXEmploy = data.norm[,"norm.a"]*data.norm[,"norm.h"]*data.norm[,"norm.h"]
x = as.matrix(data.norm[3:6],ncol=length(xName))
x = cbind(x, HealthXEmploy,AgeXPartner,AgeXHealth, AgeXEmploy,AgeXHealthXEmploy)

Nx = dim(x)[2]
Nx0 = dim(x)[2]-3 #Change for between subject madness


rand <- c()

mean1 <- runif(1,-5,5)
#Preperation

x = as.matrix(data.norm[3:6],ncol=length(xName))
x = cbind(x, AgeXHealthXEmploy)


Nx = dim(x)[2]
Nx0 = dim(x)[2]-3 #Change for between subject madness
meana <- runif(1, min=-10, max=10)
meanb <- runif(1, min=-10, max=10)

dataList.I.COR2 = list(
    x = x ,
    y = y ,
    pp = pp ,
    Ntotal = Ntotal ,
    Nsubjects = Nsubjects,
    Nx0 = Nx0,
    Nx= Nx,
    omega = diag(c(1,1)),
    mean = c(0,0),
    df = 2,
    prec = diag(c(1.0E-6,1.0E-6))
  )
```


```{r sens two thin jags}

if(!load.rather.than.run){ 
  
  parameters.C.I <- c("ss", "cor12","cor13","cor23","betamu", "betax","sigma")
  
  out.model.COR <- run.jags(model = "model_full_correlation", monitor = parameters.C.I, data = dataList.I.COR, inits = myinits, n.chains = nchains, adapt = nadapt, burnin = nburn, sample = 1000, thin = 150)

  model.1.sens <- out.model.COR 


  parameters.C.I.2.sens <- c("ss", "cor12","betamu", "betax", "sigma")
 
  out.model.COR2.sens <- run.jags(model = "model_full_correlation_2.sens", monitor = parameters.C.I.2.sens, data = dataList.I.COR2, inits = myinits, n.chains = nchains, adapt = nadapt, burnin = nburn, sample = 800, thin = 300)

  model.1.sens <- out.model.COR
}
 
if(load.rather.than.run){ 
  load("model.1.sens.rdata")
  load("model.2.sens.rdata")
}
```

```{r dic calculation}

get.dic <- function(model){
  library(runjags)
  q<-extract.runjags(model,"dic")
  
  return(mean(q$deviance)+2*mean(q$penalty))
  
}

if(!load.rather.than.run){
dic.1.main <- get.dic(model.1.main)
dic.2.main <- get.dic(model.2.main)
dic.1.sens <- get.dic(model.1.sens)
dic.2.sens <- get.dic(model.2.sens)
}

if(load.rather.than.run){
dic.1.main <- 2.2280099
dic.2.main <- 2.4545561
dic.1.sens <- 2.2277275
dic.2.sens <- 2.4493027
}
```



```{r tables of summaries, results = "asis"}

if(load.rather.than.run){
 load("summary.1.main.rdata")
 load("summary.2.main.rdata")
 load("summary.1.sens.rdata")
 load("summary.2.sens.rdata")
}

apa_table(x = summary.1.main, caption = "The first model", note = "Full model.", landscape = TRUE, small = TRUE)
apa_table(x = summary.2.main, caption = "The Second model", note = "Reduced model.", landscape = TRUE, small = TRUE)
apa_table(x = summary.1.sens, caption = "Sensitivity 1", note = "Sensitivity analysis 1.", landscape = TRUE, small = TRUE)
apa_table(x = summary.2.sens, caption = "Sensitivity 2", note = "Sensitivity analysis 2.", landscape = TRUE, small = TRUE)

```
# Research question

We explored the question of which factors are associated with life satisfaction in the german sample of $n=$`r Nsubjects` using bayesian regression techniques. Preceding the analysis, we omitted all missing values, and standardized all non-binary variables. We centered age for MCMC considerations. We also aimed to investigate whether we could construct a model that could examine whether living together has an effect on life satisfaction, when taking into account individual differences, as posited by @lucas2003reexamining. To this extent, we assured that this variable could interact with other variables of intrest, and correlated with baseline individual differences.  

# Method

We constructed a complex bayesian hierarchical regression model, with life satisfaction as dependent variable, henceforth termed $y_{i}$ and then proceeded to construct a more simple model based on the exploratory conclusions from the first. Our first model took into account the variables of health satisfaction, living together, and employment status as a between subjects factor, henceforth termed $x_{2i}$, $x_{3i}$, and $x_{4i}$ respectively. Age was included as $x_{1i}$. We included interactions between age and health satisfaction, age and partner, age and employment, and health and employment. The choices of these interactions were motivated by the fact that the ‘age’ variable changes equally for each individual, and interactions between this variable and the others seemed most likely to influence life satisfaction. Our final interaction was motivated by the idea that the association of these variables could manifest its self in concrete outcomes pertaining to work and health combinations. We used JAGS to provide us with samples from the posterior.

In all our models, we assumed ${y_{i}}$~${N(µ_{yi},\sigma)}$

with $\sigma$ ~ ${U(0.1, 1)}$

## model 1

${µ_{yi} = \beta_{0i} + \beta_{2i}x_{2i} + \beta_{3i}x_{3i} +\beta_{4}x_{4i} + \beta_{5}x_{1i}x_{2i} + \beta_{6}x_{1i}x_{3i} + \beta_{6}x_{1i}x_{4i}+\beta_{7}x_{2i}x_{4i} }$

${\beta_{0i}}$ to ${\beta_{3i}}$ ~ ${ N(\beta_{µ} , M ) }$

and ${\beta_{µ}}$ ~ ${N(0, 0.0001)}$

and ${M}$ ~ ${wish(1,4)}$

With the between subjects and interactions $\beta$'s distributed as ${N(0, \frac {1}{10^5})}$

## model 2

${µ_{yi} = \beta_{0i} + \beta_{2i}x_{2i} + \beta_{3i}x_{3i} +\beta_{4}x_{4i} + \beta_{5}x_{1i}x_{2i}x_{4i}}$

with ${\beta_{0i}}$ and ${\beta_{3i}}$ ~ ${ N(\beta_{µ} , M ) }$

and ${\beta_{µ}}$ ~ ${N(0, 0.0001)}$.

our non-correlating $\beta_{2}$ and the between subjects and interactions $\beta$'s ~ ${N(0, \frac {1}{10^5})}$.

${y_{i} = \beta_{0i} + \beta_{2i}x_{2} + \beta_{3i}x_{3} +\beta_{4}x_{4} + \beta_{5}x_{1}x_{2} + \beta_{6}x_{1}x_{3} + \beta_{7}x_{1}x_{4}+\beta_{8}x_{2}x_{4} }$

# Exploring the data

We found that age could best be conceptualised as a factor interacting with our other variables. Therefore, We plotted life satisfaction as a function of living together and age, with living together is 1 and not living together 0 (see Figure 1), life satisfaction as a function of employment and age, with full time employed as 1 and partially -or, not employed as 0 (see Figure 2) and life satisfaction as a function health status and age with perceived health status ranging from 0-10 (see Figure 3). 

# JAGS specifics

In order to assure convergence, we used a burn-in of 1000. We used 2000 iterations of the MCMC to assess the key models. For the first model, we set a thinning value of 180 to assure no autocorrelation in the sampling. For the second model, a thinning value of 200 was used. These values were based on autocorrelation plots of exploratory runs of both models. 

# Comparing the models using the Deviance Information Criterion (DIC)
To compare the trade off between complexity and data description of our models, we used the deviance information criterion (DIC). From the analysis we obtained DIC values of ‘r dic.1.main´  and ´r dic.2.main´ for the first and second model respectively. We can interprete this as following" the model with the lowest DIC value is the model with the smallest trade-off which in this case is our first model. 


# Sensitivity analysis of the models
To assess whether our models were robust against choices of our pirors, we fully randomised the means of the priors of the distributions from which our parameters were sampled, using random uniform distributions between -5 and 5. We also reduced the broadness of our precision and variance parameters, from $\frac {1}{10^5}$ to $\frac {1}{10^2}$. We found that these adaptations caused no interesting differences in our models posterior. The results of these analyses can be found in table 3 and table 4. The difference in DIC criterium between the first model and it's sensitiviy analysis was neligable(`r dic.1.main`and `r dic.1.sens`). For our second, less complex model, the DICs were also comparable (`r dic.2.main`  and `r dic.2.sens`).Furthermore, the trend in which our first model obtained the lowest DIC can also be found in the sensitivity analysis models (`r dic.1.sens` and `r dic.2.sens`).

# Discussion

Since our DIC test suggested the more complex model to provide a better trade-off between model complexity and data description, we decided to use this model to interpret how life satisfaction changes according to changes in other variables over time for different individuals. Since the SSEFF of the components are quite similar, it is easy to interpret the findings shown in Table 1. We found that our main effect variable of $\beta_{2i}$ pertaining to health satisfaction was indeed important in describing differences in life satisfaction. Interestingly, the $\beta_{4i}$ pertaining to employment status demosntrated a positive relationship between being employed at this age, and life satisfaction. This too was in accordance with our exploratory analysis of the data. An interaction plot between age, employment, and life satisfaction given our models prediction can be found in Figure 4. While Lucas, Clark, Georgellis, & Diener (2003) reported that on average, living together did not seem to have an effect on life satisfaction, we did find that our $\beta_{3i}$ value had a mean ($M_{\beta_{3i}} = 0.19$) and a standard deviation ($SD_{\beta_{3i}} = .04$), indicating an influence. However, the authors also noted that there were large individual differences in this group. By allowing individual differences in this variable, we may have captured it's effect more accurately. Furthermore, The only relatively large correlation we found between our within-subject variables was between $\beta_{0i}$ and $\beta_{3i}$, indicting that living together is correlated with other baseline individual differnces in the data. This does to some extent confirm the conclusions of @lucas2003reexamining, although we did not test this assumption directly.


\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
